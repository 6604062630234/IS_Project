import streamlit as st
import pickle
import os
import numpy as np
import pandas as pd
import tensorflow as tf
import cv2
import random
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import load_model

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .pkl
def load_pickle(file_name):
    with open(file_name, "rb") as f:
        return pickle.load(f)

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
rf_class_model = load_pickle("model_rf_class.pkl")
rf_branch_model = load_pickle("model_rf_branch.pkl")

# ‡πÇ‡∏´‡∏•‡∏î Encoders
class_encoder = load_pickle("class_encoder.pkl")
branch_encoder = load_pickle("branch_encoder.pkl")

# ‡πÇ‡∏´‡∏•‡∏î Scaler
scaler = load_pickle("scaler.pkl")

# ‡πÇ‡∏´‡∏•‡∏î Dataset Arknights
data = pd.read_csv("Arknights_Dataset.csv", encoding="latin-1")

# ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
column_descriptions = {
    "Name": "‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£",
    "ATK": "‡∏Ñ‡πà‡∏≤‡∏û‡∏•‡∏±‡∏á‡πÇ‡∏à‡∏°‡∏ï‡∏µ",
    "HP": "‡∏Ñ‡πà‡∏≤‡∏û‡∏•‡∏±‡∏á‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï",
    "DEF": "‡∏Ñ‡πà‡∏≤‡∏û‡∏•‡∏±‡∏á‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô",
    "RES": "‡∏Ñ‡πà‡∏≤‡∏ï‡πâ‡∏≤‡∏ô‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏£‡πå‡∏ï(‡∏î‡∏≤‡πÄ‡∏°‡∏à‡πÄ‡∏ß‡∏ó)",
    "Redeployment time": "‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏™‡∏ô‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
    "DP cost": "‡∏Ñ‡πà‡∏≤ DP ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏á‡∏™‡∏ô‡∏≤‡∏°",
    "Block count": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£",
    "Attack interval": "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÇ‡∏à‡∏°‡∏ï‡∏µ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£",
    "Position": "‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£",
    "Rarity": "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£",
    "Class": "‡∏Ñ‡∏•‡∏≤‡∏™‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£",
    "Branch": "‡∏™‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£"
}

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame
feature_df = pd.DataFrame(list(column_descriptions.items()), columns=["Feature", "Description"])

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï LabelEncoder
def update_label_encoder(encoder, column):
    unique_values = data[column].dropna().astype(str).unique() 
    encoder_classes = encoder.classes_.astype(str)
    
    encoder.classes_ = np.unique(np.concatenate((encoder_classes, unique_values)))
    return encoder

# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï LabelEncoders
class_encoder = pickle.load(open("class_encoder.pkl", "rb"))
branch_encoder = pickle.load(open("branch_encoder.pkl", "rb"))

class_encoder = update_label_encoder(class_encoder, "Class")
branch_encoder = update_label_encoder(branch_encoder, "Branch")

#**************************************************************************************************************************************************#

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå dataset
dataset_folder = "Hairstyles"

# ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏£‡∏á‡∏ú‡∏°‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å
if os.path.exists(dataset_folder):
    hairstyle_classes = sorted([d for d in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, d))])
else:
    hairstyle_classes = []

# ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô dataset ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö label
image_paths = []
for hairstyle in hairstyle_classes:
    folder_path = os.path.join(dataset_folder, hairstyle)
    image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    image_paths.extend(image_files)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Encoder ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
hairstyle_encoder = LabelEncoder()
if len(hairstyle_classes) > 0:  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error
    hairstyle_encoder.fit(hairstyle_classes)

encoder_path = "hairstyle_encoder.pkl"
with open(encoder_path, "wb") as f:
    pickle.dump(hairstyle_encoder, f)

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network
nn_model = load_model("neural_model.h5", compile=False)

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
def preprocess_image(image):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏Ç‡∏≤‡∏ß‡∏î‡∏≥
    image = cv2.resize(image, (128, 128))  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î
    image = np.expand_dims(image, axis=-1)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô (128, 128, 1)
    image = image.astype("float32") / 255.0  # Normalize
    image = np.expand_dims(image, axis=0)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ (1, 128, 128, 1)
    return image

#**************************************************************************************************************************************************#

# Sidebar Navigation
st.sidebar.title("Navigation Bar")
page = st.sidebar.radio("", [
    "Arknights Class and Branch Prediction (ML Model)",
    "Arknights Class and Branch Prediction (Demo)",
    "Anime Girl Hairstyle Prediction (Neural Network Model)",
    "Anime Girl Hairstyle Prediction (Demo)"
])

# ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
if page == "Arknights Class and Branch Prediction (ML Model)":
    
    col1, col2 = st.columns([3, 3]) 

    with col1:
        st.title("Machine Learning Model Detail")
    with col2:
        st.image("ML/arknights_logo.png", width=400)  
    
    st.markdown("""

---

### Introduction to Arknights   
Arknights ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏°‡πÅ‡∏ô‡∏ß Tower Defense + Strategy RPG ‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏î‡∏¢ Hypergryph ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢ Yostar ‡πÄ‡∏õ‡∏¥‡∏î‡∏ï‡∏±‡∏ß‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏à‡∏µ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏µ 2019 ‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πà‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å‡πÉ‡∏ô‡∏õ‡∏µ 2020
\n‡πÉ‡∏ô‡πÄ‡∏Å‡∏°‡∏ô‡∏µ‡πâ ‡∏ú‡∏π‡πâ‡πÄ‡∏•‡πà‡∏ô‡∏à‡∏∞‡∏£‡∏±‡∏ö‡∏ö‡∏ó‡πÄ‡∏õ‡πá‡∏ô Doctor ‡∏ú‡∏π‡πâ‡∏ô‡∏≥‡∏Ç‡∏≠‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ Rhodes Island, ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏´‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏à‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≠‡∏™‡∏π‡πâ‡∏Å‡∏±‡∏ö Originium Infection, ‡πÇ‡∏£‡∏Ñ‡∏õ‡∏£‡∏¥‡∏®‡∏ô‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÑ‡∏õ‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å Terra
                
---

### Model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Class ‡πÅ‡∏•‡∏∞ Branch ‡∏Ç‡∏≠‡∏á Operators ‡πÉ‡∏ô‡πÄ‡∏Å‡∏° Arknights ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Stats ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£

### ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á Dataset: Arknights Terra Wiki

**Dataset Details:**
- ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πÄ‡∏ï‡∏ï‡∏±‡∏™‡∏Ç‡∏≠‡∏á Operator ‡πÉ‡∏ô‡πÄ‡∏Å‡∏° Arknights ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 336 records
- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Class ‡πÅ‡∏•‡∏∞ Branch ‡∏Ç‡∏≠‡∏á Operators""")

    st.table(feature_df)

    st.markdown("""

---
                
### ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Train

### Random Forest Classifier (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Demo)
\nRandom Forest ‡πÄ‡∏õ‡πá‡∏ô ensemble learning method ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ ‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (Decision Trees) ‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡∏ã‡∏∂‡πà‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ overfitting ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Random Forest
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ (Decision Trees) ‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏∏‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤ (Bootstrap Sampling)
- ‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡πâ‡∏ô ‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á
- ‡πÉ‡∏ä‡πâ ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏ß‡∏ï‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏Å (Majority Voting) ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡∏≠‡∏á Classification
- ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (Averaging) ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡∏≠‡∏á Regression
                
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Random Forest
- ‡∏•‡∏î overfitting ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô Decision Tree
- ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ noise
- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ feature ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡πÑ‡∏î‡πâ‡∏î‡∏µ
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà ‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô (Missing Data)
                
‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á Random Forest
- ‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏π‡∏á ‡πÅ‡∏•‡∏∞‡∏ä‡πâ‡∏≤ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ
- ‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° (Interpretability) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ

---

### Support Vector Machine (SVM)
\nSupport Vector Machine (SVM) ‡πÄ‡∏õ‡πá‡∏ô ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Hyperplane) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™ ‡πÇ‡∏î‡∏¢‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á (Margin) ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏•‡∏≤‡∏™

‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á SVM
- ‡∏´‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Hyperplane) ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ margin ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
- ‡πÉ‡∏ä‡πâ Support Vectors (‡∏à‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á Hyperplane
- ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏ö‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á (Linear Separable) ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Kernel Trick ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏°‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô

‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á SVM
- ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å
- ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ú‡πà‡∏≤‡∏ô Kernel Trick
- ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÉ‡∏ô high-dimensional space
                
‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á SVM
- ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ train ‡∏ô‡∏≤‡∏ô ‡πÄ‡∏°‡∏∑‡πà‡∏≠ dataset ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
- ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏Ñ‡πà‡∏≤ Hyperparameter ‡πÄ‡∏ä‡πà‡∏ô C ‡πÅ‡∏•‡∏∞ Gamma ‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
- ‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° (Interpretability) ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏¢‡∏≤‡∏Å
                
---

### ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î      

""")
    
    st.write("Import Library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:")
    st.image("ML/import.png", width=700)

    st.write("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Dataset) ‡∏Ç‡∏∂‡πâ‡∏ô Colab:")
    st.image("ML/upload_data.png", width=700)

    st.write("‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Dataset:")
    st.image("ML/load_data.png", width=700)

    st.write("‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö Missing Value ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡πà‡∏≤:")
    st.image("ML/handle_missing.png", width=700)
    st.write("‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£:")
    st.image("ML/use_mean.png", caption="‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Mean ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£", width=700)
    st.image("ML/use_mode.png", caption="‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Mode ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£", width=700)

    st.write("‡∏™‡∏£‡πâ‡∏≤‡∏á Encoder ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Class ‡πÅ‡∏•‡∏∞ Branch (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠) ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Encode column ‡∏≠‡∏∑‡πà‡∏ô‡πÜ:")
    st.image("ML/encoder.png", width=700)

    st.write("‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:")
    st.image("ML/define_feature.png", width=700)

    st.write("‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ NaN (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ):")
    st.image("ML/handle_nan.png", width=700)

    st.write("Scaling (‡πÉ‡∏ä‡πâ Standard Scaler):")
    st.image("ML/scaling.png", width=700)

    st.write("Train Random Forest Model:")
    st.image("ML/rf_train.png", width=700)

    st.write("Train SVM Model:")
    st.image("ML/svm_train.png", width=700)

    st.write("‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Train:")
    st.image("ML/evaluate.png", width=700)
    st.image("ML/test_result.png", caption="‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Train", width=700)

    st.write("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÅ‡∏•‡∏∞ Encoder:")
    st.image("ML/save_model.png", width=700)

    st.markdown("""---""")

    st.markdown("""### Demo: Arknights Class & Branch Prediction""")
    st.write("‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Demo ‡∏Ç‡∏≠‡∏á ML Model ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° '‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£ 1 ‡∏ï‡∏±‡∏ß ‡∏à‡∏≤‡∏Å Dataset ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Class ‡πÅ‡∏•‡∏∞ Branch ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡∏ô‡∏±‡πâ‡∏ô ‡∏î‡∏±‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ")
    st.image("ML/arknights_demo.png", width=700)

    st.markdown("""
                
---
                
## ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•
- Dataset ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ
- ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö Missing Value
- ‡∏•‡∏≠‡∏á Train ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                
""")

#**************************************************************************************************************************************************#

# ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
elif page == "Arknights Class and Branch Prediction (Demo)":
    st.title("Arknights Class & Branch Prediction (Demo)")

    if st.button("üé≤ ‡∏™‡∏∏‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£"):
        while True:
            random_idx = np.random.randint(0, len(data))
            sample = data.iloc[[random_idx]]

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Missing
            if sample.isnull().sum().sum() == 0:
                break

        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á (True Labels)
        y_true_class = sample["Class"].values[0]
        y_true_branch = sample["Branch"].values[0]

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Features ‡πÇ‡∏î‡∏¢‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        X_sample = sample.drop(columns=["Name", "Class", "Branch"], errors="ignore")

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Categorical -> ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        for col in X_sample.columns:
            if X_sample[col].dtype == "object":
                unique_values = list(class_encoder.classes_)  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô
                new_values = X_sample[col].unique()  # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡πà‡∏°‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà

                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô LabelEncoder ‡πÅ‡∏•‡∏∞ fit ‡πÉ‡∏´‡∏°‡πà
                unseen_values = [val for val in new_values if val not in unique_values]
                if unseen_values:
                    class_encoder.classes_ = np.concatenate((class_encoder.classes_, unseen_values))

                # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ error
                X_sample[col] = class_encoder.transform(X_sample[col].astype(str))

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÅ‡∏ï‡πà‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        X_sample = X_sample.select_dtypes(include=[np.number])

        # ‡∏ó‡∏≥ Scaling ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        X_sample = scaler.transform(X_sample)

        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        pred_class_rf = rf_class_model.predict(X_sample)[0]
        pred_branch_rf = rf_branch_model.predict(X_sample)[0]

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
        pred_class_rf = class_encoder.inverse_transform([pred_class_rf])[0]
        pred_branch_rf = branch_encoder.inverse_transform([pred_branch_rf])[0]

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        st.write("## üé≠ Sample Character Information")
        st.write(sample)

        st.write("### ‚úÖ True Label")
        st.write(f"**Class:** {y_true_class}")
        st.write(f"**Branch:** {y_true_branch}")

        st.write("### ü§ñ Model Prediction")
        st.write(f"**Class:** {pred_class_rf}")
        st.write(f"**Branch:** {pred_branch_rf}")

#**************************************************************************************************************************************************#

# ‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• Anime Hairstyle
elif page == "Anime Girl Hairstyle Prediction (Neural Network Model)":
    
    col1, col2 = st.columns([3, 3]) 

    with col1:
        st.title("Neural Network Model Detail")
    with col2:
        st.image("NN/Title.png", width=500)
    
    st.markdown("""
                
---
                
### Model ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ó‡∏£‡∏á‡∏ú‡∏°‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞(‡∏´‡∏ç‡∏¥‡∏á)‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û

### ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Ç‡∏≠‡∏á Dataset: Deepai (Image Generator AI)

**Dataset Details:**
- ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞(‡∏´‡∏ç‡∏¥‡∏á) ‡πÉ‡∏ô‡∏ó‡∏£‡∏á‡∏ú‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 2,600 ‡∏£‡∏π‡∏õ (‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏Ç‡∏≤‡∏ß-‡∏î‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
- ‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ï‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏ú‡∏° ‡πÇ‡∏î‡∏¢‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 5 ‡∏Ñ‡∏•‡∏≤‡∏™ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà twintail(‡∏ó‡∏ß‡∏¥‡∏ô‡πÄ‡∏ó‡∏•), ponytail(‡∏´‡∏≤‡∏á‡∏°‡πâ‡∏≤), short(‡∏ú‡∏°‡∏™‡∏±‡πâ‡∏ô), long(‡∏ú‡∏°‡∏¢‡∏≤‡∏ß), braid(‡∏ú‡∏°‡πÄ‡∏õ‡∏µ‡∏¢)
- ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û 500 - 550 ‡∏£‡∏π‡∏õ
- ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ó‡∏£‡∏á‡∏ú‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û

---
                
### ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Train

### Convolutional Neural Network (CNN)
\nConvolutional Neural Network (CNN) ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Ç‡πà‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á CNN ‡∏°‡∏µ‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

- Convolution Layer ‚Üí ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏™‡πâ‡∏ô ‡∏Ç‡∏≠‡∏ö ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡∏ó‡∏£‡∏á
- Pooling Layer ‚Üí ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
- Fully Connected Layer ‚Üí ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå
- Activation Function (ReLU, Softmax) ‚Üí ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

CNN ‡∏à‡∏∂‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏û
                
---

### ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î      

""")
    
    st.write("Import Library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:")
    st.image("NN/import.png", width=700)

    st.write("‡∏î‡∏∂‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å Google Drive:")
    st.image("NN/drive.png", width=700)

    st.write("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hyperparameters:")
    st.image("NN/set_parameters.png", width=700)

    st.write("‡∏£‡∏µ‡∏™‡πÄ‡∏Å‡∏• ‡πÅ‡∏•‡∏∞ split ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
    st.image("NN/split.png", width=700)
    st.write("‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train ‡πÅ‡∏•‡∏∞ Validation")
    st.image("NN/load.png", caption="‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Mean ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£", width=700)

    st.write("‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Class ‡∏Ç‡∏≠‡∏á‡∏ó‡∏£‡∏á‡∏ú‡∏°:")
    st.image("NN/check_class.png", width=700)

    st.write("Build ‡πÇ‡∏°‡πÄ‡∏î‡∏•:")
    st.image("NN/build_model.png", width=700)

    st.write("Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•:")
    st.image("NN/compile.png", width=700)

    st.write("Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•:")
    st.image("NN/train_model.png", width=700)
    st.image("NN/train_acc.png", caption="Train Result", width=700)

    st.write("‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Train:")
    st.image("NN/train_result.png", width=700)

    st.write("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•:")
    st.image("NN/save_model.png", width=700)

    st.markdown("""
                
---
                
## ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•
- Dataset ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ
- ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ Data Augmentation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Overfitting
- ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏µ(RGB) ‡πÉ‡∏ô Dataset
                
""")

#**************************************************************************************************************************************************#

# ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå Anime Hairstyle
elif page == "Anime Girl Hairstyle Prediction (Demo)":
    st.title("Anime Girl Hairstyle Prediction (Demo)")
    
    # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    uploaded_file = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì", type=["jpg", "png", "jpeg"])

    # ‡∏™‡∏∏‡πà‡∏°‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å dataset
    selected_image_path = None
    if image_paths:
        if st.button("üé≤ ‡∏™‡∏∏‡πà‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"):
            selected_image_path = random.choice(image_paths)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏∏‡πà‡∏°‡∏°‡∏≤
    image = None
    if uploaded_file is not None:
        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
        image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
        st.image(image, caption="Uploaded Image", width=700, channels="BGR")

    elif selected_image_path:
        image = cv2.imread(selected_image_path, cv2.IMREAD_COLOR)
        st.image(image, caption=f"Random Image: {os.path.basename(selected_image_path)}", width=700, channels="BGR")

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    if image is not None:
        processed_image = preprocess_image(image)
        prediction = nn_model.predict(processed_image)
        predicted_label = np.argmax(prediction)

        if len(hairstyle_encoder.classes_) > predicted_label:
            predicted_hairstyle = hairstyle_encoder.inverse_transform([predicted_label])[0]
        else:
            predicted_hairstyle = "Unknown"

        confidence = np.max(prediction) * 100  

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        st.write("### ü§ñ Model Prediction")
        st.write(f"**Hairstyle:** {predicted_hairstyle}")
        st.write(f"**Confidence:** {confidence:.2f}%")